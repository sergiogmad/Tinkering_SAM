{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of contents\n### - [Introduction](#Introduction)\n### - [Setup](#Setup)\n### - [Example image](#Example-image)\n### - [Automatic mask generation](#Automatic-mask-generation)\n### - [Automatic mask generation options](#Automatic-mask-generation-options)\n### - [Classification model](#Classification-model)\n### - [Inference](#Inference)\n### - [Gradio app](#Gradio-app)\n### - [References](#References)\n### - [Limitations](#Limitations)","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I will combine Meta's segmentation model called **`SAM`** (Segment Anything Model) with a pretrained image classification model from the timm library to create a simple Gradio app. This app will categorize the objects found in an input image.\n\nA significant portion of the code and explanations presented here has been replicated and adapted from the sources detailed in the [References](#References) section.\n\nThe purpose of this modest project is solely to experiment, learn, and practice.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport sys\n!{sys.executable} -m pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\nimport torch\nimport torchvision\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\n!pip install -q gradio\nimport requests\n\n!mkdir images\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n\n# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_4b8939.pth","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.964677Z","iopub.status.idle":"2023-08-03T19:55:14.965895Z","shell.execute_reply.started":"2023-08-03T19:55:14.965503Z","shell.execute_reply":"2023-08-03T19:55:14.965533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.967985Z","iopub.status.idle":"2023-08-03T19:55:14.969227Z","shell.execute_reply.started":"2023-08-03T19:55:14.968823Z","shell.execute_reply":"2023-08-03T19:55:14.968860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = './images/dog.jpg'","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.971206Z","iopub.status.idle":"2023-08-03T19:55:14.972025Z","shell.execute_reply.started":"2023-08-03T19:55:14.971759Z","shell.execute_reply":"2023-08-03T19:55:14.971784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam_model_path = '../input/segment-anything/pytorch/vit-b/1/model.pth'","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.973475Z","iopub.status.idle":"2023-08-03T19:55:14.974277Z","shell.execute_reply.started":"2023-08-03T19:55:14.974025Z","shell.execute_reply":"2023-08-03T19:55:14.974050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    ax.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.975702Z","iopub.status.idle":"2023-08-03T19:55:14.976480Z","shell.execute_reply.started":"2023-08-03T19:55:14.976235Z","shell.execute_reply":"2023-08-03T19:55:14.976258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example image","metadata":{}},{"cell_type":"code","source":"image = Image.open(image_path)\nimage = np.array(image.convert(\"RGB\"))\nplt.figure(figsize=(5,5))\nplt.imshow(image)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.977893Z","iopub.status.idle":"2023-08-03T19:55:14.979111Z","shell.execute_reply.started":"2023-08-03T19:55:14.978711Z","shell.execute_reply":"2023-08-03T19:55:14.978749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic mask generation","metadata":{}},{"cell_type":"markdown","source":"To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended.","metadata":{}},{"cell_type":"code","source":"# import sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nsam_checkpoint = sam_model_path\nmodel_type = \"vit_b\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device);\n\n# mask_generator = SamAutomaticMaskGenerator(sam)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.981231Z","iopub.status.idle":"2023-08-03T19:55:14.982293Z","shell.execute_reply.started":"2023-08-03T19:55:14.982070Z","shell.execute_reply":"2023-08-03T19:55:14.982092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To generate masks, just run `generate` on an image.","metadata":{}},{"cell_type":"code","source":"# masks = mask_generator.generate(image)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.983621Z","iopub.status.idle":"2023-08-03T19:55:14.984403Z","shell.execute_reply.started":"2023-08-03T19:55:14.984144Z","shell.execute_reply":"2023-08-03T19:55:14.984166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show all the masks overlayed on the image","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(5,5))\n# plt.imshow(image)\n# show_anns(masks)\n# plt.axis('off')\n# plt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.985919Z","iopub.status.idle":"2023-08-03T19:55:14.993530Z","shell.execute_reply.started":"2023-08-03T19:55:14.993138Z","shell.execute_reply":"2023-08-03T19:55:14.993172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic mask generation options","metadata":{}},{"cell_type":"markdown","source":"There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:","metadata":{}},{"cell_type":"code","source":"mask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.995686Z","iopub.status.idle":"2023-08-03T19:55:14.996894Z","shell.execute_reply.started":"2023-08-03T19:55:14.996522Z","shell.execute_reply":"2023-08-03T19:55:14.996557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks2 = mask_generator_2.generate(image)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:14.998911Z","iopub.status.idle":"2023-08-03T19:55:15.000079Z","shell.execute_reply.started":"2023-08-03T19:55:14.999713Z","shell.execute_reply":"2023-08-03T19:55:14.999749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(masks2)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.002175Z","iopub.status.idle":"2023-08-03T19:55:15.003359Z","shell.execute_reply.started":"2023-08-03T19:55:15.002964Z","shell.execute_reply":"2023-08-03T19:55:15.002999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks2[0].keys()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.012494Z","iopub.status.idle":"2023-08-03T19:55:15.013675Z","shell.execute_reply.started":"2023-08-03T19:55:15.013309Z","shell.execute_reply":"2023-08-03T19:55:15.013345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show all the masks overlayed on the image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(image)\nshow_anns(masks2)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.015715Z","iopub.status.idle":"2023-08-03T19:55:15.016876Z","shell.execute_reply.started":"2023-08-03T19:55:15.016508Z","shell.execute_reply":"2023-08-03T19:55:15.016543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show the dog mask in isolation, with dark background","metadata":{}},{"cell_type":"code","source":"mask = np.where(masks2[23]['segmentation'], 1, 0)\nnew_image = image * np.expand_dims(mask, axis=-1)\nplt.figure(figsize=(5,5))\nplt.imshow(new_image)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.018920Z","iopub.status.idle":"2023-08-03T19:55:15.020083Z","shell.execute_reply.started":"2023-08-03T19:55:15.019716Z","shell.execute_reply":"2023-08-03T19:55:15.019752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show each mask separately","metadata":{}},{"cell_type":"code","source":"# for i in range(len(masks2)):\n#     print(i)\n#     mask = np.where(masks2[i]['segmentation'], 1, 0)\n#     new_image = image * np.expand_dims(mask, axis=-1)\n#     plt.figure(figsize=(5,5))\n#     plt.imshow(new_image)\n#     plt.axis('off')\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.024382Z","iopub.status.idle":"2023-08-03T19:55:15.025538Z","shell.execute_reply.started":"2023-08-03T19:55:15.025177Z","shell.execute_reply":"2023-08-03T19:55:15.025214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to determine whether a mask belongs to a greater one","metadata":{}},{"cell_type":"code","source":"def is_part_of(mask1, mask2):\n    mask1, mask2 = mask1.flatten(), mask2.flatten()\n    # True if mask1 is entirely contained within mask2\n    if np.any(mask1 & ~mask2):\n        return False\n    # True if both masks are NOT identical    \n    if np.count_nonzero(mask1)==np.count_nonzero(mask2):\n        return False\n    return True\n\n\n# check objects belonging to mask23\n\nfor i in range(len(masks2)):\n    if is_part_of(masks2[i]['segmentation'], masks2[23]['segmentation']):\n        print(f\"Mask {i} is part of mask23\")\n\n\n# check if mask21 belongs to another mask\n\nfor i in range(len(masks2)):\n    if is_part_of(masks2[21]['segmentation'], masks2[i]['segmentation']):\n        print(f\"Mask 21 is part of mask {i}.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.034223Z","iopub.status.idle":"2023-08-03T19:55:15.035358Z","shell.execute_reply.started":"2023-08-03T19:55:15.034992Z","shell.execute_reply":"2023-08-03T19:55:15.035043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification model","metadata":{}},{"cell_type":"code","source":"# !pip install timm (not necessary in Kaggle)\nimport timm\n# timm.list_models('*huge*in1k*', pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.036795Z","iopub.status.idle":"2023-08-03T19:55:15.037784Z","shell.execute_reply.started":"2023-08-03T19:55:15.037387Z","shell.execute_reply":"2023-08-03T19:55:15.037425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chose model blablabla because blablabla","metadata":{}},{"cell_type":"code","source":"cv_model = timm.create_model('convnext_small.in12k_ft_in1k_384', pretrained=True).eval()\ntransform = timm.data.create_transform(\n    **timm.data.resolve_data_config(cv_model.pretrained_cfg)\n)\ntransform\n# 86.182\t97.92\t384---\t50.22\t25.58\t63.37\t516.19\t256","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.040079Z","iopub.status.idle":"2023-08-03T19:55:15.041254Z","shell.execute_reply.started":"2023-08-03T19:55:15.040869Z","shell.execute_reply":"2023-08-03T19:55:15.040904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply transform to image prior to inference","metadata":{}},{"cell_type":"code","source":"pil_image = Image.fromarray(new_image.astype(np.uint8))\nimage_tensor = transform(pil_image)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.043303Z","iopub.status.idle":"2023-08-03T19:55:15.044486Z","shell.execute_reply.started":"2023-08-03T19:55:15.044094Z","shell.execute_reply":"2023-08-03T19:55:15.044118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show transformed image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(np.transpose(image_tensor, (1,2,0)))\nshow_anns(masks2)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.053435Z","iopub.status.idle":"2023-08-03T19:55:15.054577Z","shell.execute_reply.started":"2023-08-03T19:55:15.054222Z","shell.execute_reply":"2023-08-03T19:55:15.054259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    output = cv_model(image_tensor.unsqueeze(0))\noutput.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.056570Z","iopub.status.idle":"2023-08-03T19:55:15.057733Z","shell.execute_reply.started":"2023-08-03T19:55:15.057365Z","shell.execute_reply":"2023-08-03T19:55:15.057401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = torch.nn.functional.softmax(output[0], dim=0)\nprobabilities.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.059841Z","iopub.status.idle":"2023-08-03T19:55:15.061069Z","shell.execute_reply.started":"2023-08-03T19:55:15.060697Z","shell.execute_reply":"2023-08-03T19:55:15.060732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values, indices = torch.topk(probabilities, 3)\nindices","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.064793Z","iopub.status.idle":"2023-08-03T19:55:15.071379Z","shell.execute_reply.started":"2023-08-03T19:55:15.071095Z","shell.execute_reply":"2023-08-03T19:55:15.071118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtain labels from the `IMAGENET_1k` dataset","metadata":{}},{"cell_type":"code","source":"IMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\nIMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')\nprint('The 3 most likely labels for the given picture are:', '\\n')\n[{'label': IMAGENET_1k_LABELS[idx], 'value': np.round(val.item()*100, 2)} for val, idx in zip(values, indices)]","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.072795Z","iopub.status.idle":"2023-08-03T19:55:15.073359Z","shell.execute_reply.started":"2023-08-03T19:55:15.073111Z","shell.execute_reply":"2023-08-03T19:55:15.073134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------","metadata":{"execution":{"iopub.status.busy":"2023-07-17T12:24:53.600420Z","iopub.execute_input":"2023-07-17T12:24:53.600859Z","iopub.status.idle":"2023-07-17T12:24:53.611634Z","shell.execute_reply.started":"2023-07-17T12:24:53.600825Z","shell.execute_reply":"2023-07-17T12:24:53.610052Z"}}},{"cell_type":"markdown","source":"## Gradio app","metadata":{}},{"cell_type":"code","source":"# setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torchvision\nimport sys\n!{sys.executable} -m pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\nimport timm\nimport requests\n!pip install -q gradio\nimport gradio as gr\n\n# automatic mask generation\nsam_model_path = '../input/segment-anything/pytorch/vit-b/1/model.pth'\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\nsam_checkpoint = sam_model_path\nmodel_type = \"vit_b\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)\n\n# classification model\ncv_model = timm.create_model('convnext_small.in12k_ft_in1k_384', pretrained=True).eval()\ntransform = timm.data.create_transform(\n    **timm.data.resolve_data_config(cv_model.pretrained_cfg)\n)\n\n# ImageNet labels\nIMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\nIMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')\n\ndef predict(image):\n    masks2 = mask_generator_2.generate(image)\n    label_lst = []\n    for i in range(len(masks2)):\n        mask = np.where(masks2[i]['segmentation'], 1, 0)\n        new_image = image * np.expand_dims(mask, axis=-1)\n        pil_image = Image.fromarray(new_image.astype(np.uint8))\n        image_tensor = transform(pil_image)\n        \n        with torch.no_grad():\n            output = cv_model(image_tensor.unsqueeze(0))\n        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n        values, indices = torch.topk(probabilities, 1)\n        label_value_lst = [{'label': IMAGENET_1k_LABELS[idx], 'value (%)': np.round(val.item()*100, 2)} for val, idx in zip(values, indices)]\n        if label_value_lst[0]['value (%)']>10:\n            label_lst.append(label_value_lst[0]['label'])\n    #         print(f'{i} The most likely label for the given object is:', '\\n', label_value_lst)\n    #         plt.figure(figsize=(5,5))\n    #         plt.imshow(np.transpose(image_tensor, (1,2,0)))\n    #         plt.axis('off')\n    #         plt.show()\n    return f'The given picture probably contains the following items: {label_lst}'\n\ngr.Interface(fn=predict, inputs='image', outputs='text').launch(share=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T20:03:31.711329Z","iopub.execute_input":"2023-08-03T20:03:31.711885Z","iopub.status.idle":"2023-08-03T20:04:29.282569Z","shell.execute_reply.started":"2023-08-03T20:03:31.711846Z","shell.execute_reply":"2023-08-03T20:04:29.281462Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/201M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0334417b084751bb0cbdece17f8431"}},"metadata":{}},{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://804b6a3e633a7afe42.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://804b6a3e633a7afe42.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"# backlog\n    # review best practices for organizing imports\n    # gradio - comments on predict function\n    # imports, import sam model\n    # group samples in batches according to batch size -256- comment out unsqueeze\n    # url = 'https://www.closetfactory.com/wp-content/uploads/2020/01/shutterstock_1381069457.png'\n            # 'image = Image.open(requests.get(url, stream=True).raw)'\n    # explain timm model selection -model size-param_count https://huggingface.co/timm/convnext_small.in12k_ft_in1k_384\n        # top1---\ttop5----\timg_size\tparam_count\t--samples_per_sec\tbatch_size\n        # 86.182\t97.92\t----384---\t------50.22\t---------516--------\t256\n\n    # 10 biggest / most reliable objects\n    # slider control to select 5 objects by size, certainty\n    # object is part of another object...\n    # obtain image and model automatically\n    # decide sam model\n    # citation for timm model and timm's r wightman\n    # future work: ?\n    # introduction - purpose this work experimentation, learning, practicing\n    # limitations: Sam model, timm model, 1k classes imagenet. rubbish performance vs image to prompt models..diffusion..","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.077421Z","iopub.status.idle":"2023-08-03T19:55:15.078129Z","shell.execute_reply.started":"2023-08-03T19:55:15.077853Z","shell.execute_reply":"2023-08-03T19:55:15.077877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# caution","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.080266Z","iopub.status.idle":"2023-08-03T19:55:15.085388Z","shell.execute_reply.started":"2023-08-03T19:55:15.085104Z","shell.execute_reply":"2023-08-03T19:55:15.085129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aux\n\n# import pickle\n        # with open(file_path, 'wb') as file:\n            # pickle.dump(data, file)\n        # with open(file_path, 'rb') as file:\n            # loaded_data = pickle.load(file)\n        # import io; # Serialize using BytesIO and pickle\n            # buffer = io.BytesIO()\n            # pickle.dump(data, buffer)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T19:55:15.086865Z","iopub.status.idle":"2023-08-03T19:55:15.087458Z","shell.execute_reply.started":"2023-08-03T19:55:15.087209Z","shell.execute_reply":"2023-08-03T19:55:15.087235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"- Segment Anything Model (`SAM`) by Meta\n    - https://github.com/facebookresearch/segment-anything/blob/main/README.md\n\n    - @article{kirillov2023segany,\n  title={Segment Anything},\n  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\n  journal={arXiv:2304.02643},\n  year={2023}\n}\n\n- Timm Library\n    - https://timm.fast.ai/\n    - https://huggingface.co/docs/timm/quickstart","metadata":{}},{"cell_type":"markdown","source":"## Limitations","metadata":{}}]}