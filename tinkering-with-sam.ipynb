{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of contents\n### - [Setup](#Setup)\n### - [Example image](#Example-image)\n### - [Automatic mask generation](#Automatic-mask-generation)\n### - [Automatic mask generation options](#Automatic-mask-generation-options)","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport torchvision\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\nimport sys\n# !{sys.executable} -m pip install -q opencv-python\n!{sys.executable} -m pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n\n!mkdir images\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n\n#     !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_4b8939.pth","metadata":{"execution":{"iopub.status.busy":"2023-07-26T16:55:04.825384Z","iopub.execute_input":"2023-07-26T16:55:04.825825Z","iopub.status.idle":"2023-07-26T16:55:31.151915Z","shell.execute_reply.started":"2023-07-26T16:55:04.825791Z","shell.execute_reply":"2023-07-26T16:55:31.150237Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"PyTorch version: 2.0.0+cpu\nTorchvision version: 0.15.1+cpu\nCUDA is available: False\n--2023-07-26 16:55:30--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 99846 (98K) [image/jpeg]\nSaving to: ‘images/dog.jpg’\n\ndog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.009s  \n\n2023-07-26 16:55:31 (10.4 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = './images/dog.jpg'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/segment-anything/pytorch/vit-b/1/model.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    ax.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example image","metadata":{}},{"cell_type":"code","source":"image = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(image)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic mask generation","metadata":{}},{"cell_type":"markdown","source":"To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nsam_checkpoint = model_path\nmodel_type = \"vit_b\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\nmask_generator = SamAutomaticMaskGenerator(sam)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To generate masks, just run `generate` on an image.","metadata":{}},{"cell_type":"code","source":"# masks = mask_generator.generate(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n* `segmentation` : the mask\n* `area` : the area of the mask in pixels\n* `bbox` : the boundary box of the mask in XYWH format\n* `predicted_iou` : the model's own prediction for the quality of the mask\n* `point_coords` : the sampled input point that generated this mask\n* `stability_score` : an additional measure of mask quality\n* `crop_box` : the crop of the image used to generate this mask in XYWH format","metadata":{}},{"cell_type":"code","source":"# print(len(masks))\n# print(masks[0].keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show all the masks overlayed on the image.","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(5,5))\n# plt.imshow(image)\n# show_anns(masks)\n# plt.axis('off')\n# plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic mask generation options","metadata":{}},{"cell_type":"markdown","source":"There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:","metadata":{}},{"cell_type":"code","source":"mask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks2 = mask_generator_2.generate(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(masks2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(5,5))\n# plt.imshow(image)\n# show_anns(masks2)\n# plt.axis('off')\n# plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.where(masks2[23]['segmentation'], 1, 0)\nnew_image = image * np.expand_dims(mask, axis=-1)\n# new_image.shape\nplt.figure(figsize=(5,5))\nplt.imshow(new_image)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show each mask separately\n# for i in range(len(masks2)):\n#     print(i)\n#     mask = np.where(masks2[i]['segmentation'], 1, 0)\n#     new_image = image * np.expand_dims(mask, axis=-1)\n#     plt.figure(figsize=(5,5))\n#     plt.imshow(new_image)\n#     plt.axis('off')\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def is_part_of(mask1, mask2):\n#     mask1, mask2 = mask1.flatten(), mask2.flatten()\n#     # Check if mask1 is entirely contained within mask2\n#     for m1, m2 in zip(mask1, mask2):\n#         if m1 == 1 and m2 == 0:\n#             return False\n#     if np.count_nonzero(mask1)==np.count_nonzero(mask2):\n#             return False\n#     print(True, i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check objects belonging to mask23\n# for i in range(len(masks2)):\n#     is_part_of(masks2[i]['segmentation'], masks2[23]['segmentation'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check if mask21 belongs to another mask\n# for i in range(len(masks2)):\n#     is_part_of(masks2[21]['segmentation'], masks2[i]['segmentation'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install timm (not necessary in Kaggle)\nimport timm","metadata":{"execution":{"iopub.status.busy":"2023-07-26T09:00:23.005375Z","iopub.execute_input":"2023-07-26T09:00:23.005803Z","iopub.status.idle":"2023-07-26T09:00:23.012456Z","shell.execute_reply.started":"2023-07-26T09:00:23.005768Z","shell.execute_reply":"2023-07-26T09:00:23.010811Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"timm.list_models('convnext*tin*', pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T09:00:27.486277Z","iopub.execute_input":"2023-07-26T09:00:27.486735Z","iopub.status.idle":"2023-07-26T09:00:27.500172Z","shell.execute_reply.started":"2023-07-26T09:00:27.486700Z","shell.execute_reply":"2023-07-26T09:00:27.498731Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['convnext_tiny.fb_in1k',\n 'convnext_tiny.fb_in22k',\n 'convnext_tiny.fb_in22k_ft_in1k',\n 'convnext_tiny.fb_in22k_ft_in1k_384',\n 'convnext_tiny.in12k',\n 'convnext_tiny.in12k_ft_in1k',\n 'convnext_tiny.in12k_ft_in1k_384',\n 'convnext_tiny_hnf.a2h_in1k',\n 'convnextv2_tiny.fcmae',\n 'convnextv2_tiny.fcmae_ft_in1k',\n 'convnextv2_tiny.fcmae_ft_in22k_in1k',\n 'convnextv2_tiny.fcmae_ft_in22k_in1k_384']"},"metadata":{}}]},{"cell_type":"code","source":"cv_model = timm.create_model('convnext_tiny.fb_in1k', pretrained=True).eval()\ntransform = timm.data.create_transform(\n    **timm.data.resolve_data_config(cv_model.pretrained_cfg)\n)\n\ntransform","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\npil_image = Image.fromarray(new_image.astype(np.uint8))\nimage_tensor = transform(pil_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = cv_model(image_tensor.unsqueeze(0))\noutput.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = torch.nn.functional.softmax(output[0], dim=0)\nprobabilities.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values, indices = torch.topk(probabilities, 3)\nindices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nIMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\nIMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')\n[{'label': IMAGENET_1k_LABELS[idx], 'value': np.round(val.item()*100, 2)} for val, idx in zip(values, indices)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(np.transpose(image_tensor, (1,2,0)))\n# show_anns(masks2)\n# plt.axis('off')\nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------","metadata":{"execution":{"iopub.status.busy":"2023-07-17T12:24:53.600420Z","iopub.execute_input":"2023-07-17T12:24:53.600859Z","iopub.status.idle":"2023-07-17T12:24:53.611634Z","shell.execute_reply.started":"2023-07-17T12:24:53.600825Z","shell.execute_reply":"2023-07-17T12:24:53.610052Z"}}},{"cell_type":"code","source":"#algorithm for detecting and describing objects found in a given picture\n\n#initial setup, imports\n\n    # remember to change model install and to automate image_path and model path!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!        \nimport torch\nimport torchvision\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\nimport sys\n!{sys.executable} -m pip install -q opencv-python matplotlib\n!{sys.executable} -m pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n\n!mkdir images\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport timm\nfrom PIL import Image\n\n#image and model already in image_path, model_path\nimage_path = './images/dog.jpg'\nmodel_path = '../input/segment-anything/pytorch/vit-b/1/model.pth'\n\n#preparing image for cv2 (mask post-processing)\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#preparing SAM automatic mask generator\nimport sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nsam_checkpoint = model_path\nmodel_type = \"vit_b\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\nmask_generator = SamAutomaticMaskGenerator(sam)\n\n#preparing masks2\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)\nmasks2 = mask_generator_2.generate(image)\n\n#prepare cv_model and transform for inference\ncv_model = timm.create_model('convnext_tiny.fb_in1k', pretrained=True).eval()\ntransform = timm.data.create_transform(\n    **timm.data.resolve_data_config(cv_model.pretrained_cfg)\n)\n\n#ImageNet lookup table ready\nimport requests\nIMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\nIMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')\n\nfor i in range(len(masks2)):\n    #convert mask to image\n    mask = np.where(masks2[i]['segmentation'], 1, 0)\n    new_image = image * np.expand_dims(mask, axis=-1)\n    #convert image to pil to prep for transform\n    pil_image = Image.fromarray(new_image.astype(np.uint8))\n    #apply transform to prep for inference\n    image_tensor = transform(pil_image)\n    #inference\n    output = cv_model(image_tensor.unsqueeze(0))\n    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n    values, indices = torch.topk(probabilities, 1)\n    lst = [{'label': IMAGENET_1k_LABELS[idx], 'value in %': np.round(val.item()*100, 2)} for val, idx in zip(values, indices) if val>0.15]\n    if len(lst):\n        # print predicted classes w/ probs\n        print(f'mask{i}', lst)\n        # show masks meeting criteria\n        plt.figure(figsize=(5,5))\n        plt.imshow(np.transpose(image_tensor, (1,2,0)))\n        plt.axis('off')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T12:21:04.680595Z","iopub.execute_input":"2023-07-17T12:21:04.680963Z","iopub.status.idle":"2023-07-17T12:22:28.143278Z","shell.execute_reply.started":"2023-07-17T12:21:04.680931Z","shell.execute_reply":"2023-07-17T12:22:28.142172Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# backlog\n    # index, hyperlinks\n    # object is part of another object...\n    # obtain image and model automatically\n    # change sam model\n    # change cv model\n    # precision/recall curve to compare output quality","metadata":{},"execution_count":null,"outputs":[]}]}